{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "%matplotlib inline\n",
    "import os\n",
    "# 设置环境变量（放在import tf前才生效）\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.keras as keras\n",
    "# import tensorflow._api.v2.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from glob import glob\n",
    "import pickle\n",
    "from tqdm import tqdm # 进度条库\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from python_speech_features import mfcc # 提取音频特征，用pip安装\n",
    "import librosa # 音频处理 用conda安装\n",
    "from IPython.display import Audio\n",
    "import wave\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 tensorflow-gpu\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available()) # 提示使用新方法\n",
    "# print(tf.config.list_physical_devices('GPU')) # 新方法查看可用GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 设置GPU\n",
    "\n",
    "from warnings import simplefilter\n",
    "\n",
    "# 设置GPU定量分配\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8  # 占用GPU90%的显存\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# 设置GPU按需分配\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取指定路径下所有的pcm文件\n",
    "\n",
    "files = glob(r'D:\\Users\\GraduationDesign\\AudioProcessed\\*\\*.wav')\n",
    "\n",
    "train_files, vad_files = train_test_split(files, test_size=0.99, random_state=40)\n",
    "\n",
    "train_files, vad_files = train_test_split(train_files, test_size=0.2, random_state=40)\n",
    "\n",
    "print(len(train_files), len(vad_files), train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'train': [], 'vad': []} # 训练集和验证集\n",
    "\n",
    "for i in tqdm(range( len(train_files) )):\n",
    "    path = train_files[i]\n",
    "    label = path.split('\\\\')[4] # 3\n",
    "    labels['train'].append(label)\n",
    "\n",
    "for i in tqdm(range( len(vad_files) )):\n",
    "    path = vad_files[i]\n",
    "    label = path.split('\\\\')[4]\n",
    "    labels['vad'].append(label)\n",
    "\n",
    "print(len(labels['train']), len(labels['vad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfcc_dim = 13       # 通道数为13\n",
    "sr = 16000          # 采样率为16000\n",
    "min_length = 1 * sr\n",
    "slice_length = 3 * sr\n",
    "\n",
    "\n",
    "#定义加载并处理语音函数\n",
    "def load_and_trim(path, sr=16000):                  # memmap对象，它允许将大文件分成小段进行读写，而不是一次性将整个数组读入内存。\n",
    "    # audio = np.memmap(path, dtype='h', mode='r')  # 使用函数np.memmap并传入一个文件路径、数据类型、形状以及文件模式\n",
    "    f = wave.open(path, 'rb')\n",
    "    \n",
    "    params = f.getparams()\n",
    "    nchannels,sampwidth,framerate,nframers = params[:4]\n",
    "    str_data = f.readframes(nframers)\n",
    "    f.close()\n",
    "    audio = np.frombuffer(str_data, dtype='h')  # 从fromstring修改\n",
    "    \n",
    "    audio = audio[2000:-2000]                           # b = a[i:j] 表示复制a[i]到a[j-1]，为负数时表示倒数几个，以生成新的list对象\n",
    "    audio = audio.astype(np.float32)                    # astype()：类型转换为float32\n",
    "    energy = librosa.feature.rms(y=audio)                 # 调用librosa中的rmse直接对音频每个帧进行计算得到均方根能量\n",
    "    frames = np.nonzero(energy >= np.max(energy) / 5)   # 返回大于五分之一最大能量的能量索引\n",
    "    indices = librosa.core.frames_to_samples(frames)[1] # librosa.core.frames_to_samples():将帧索引转换为音频样本索引\n",
    "    audio = audio[indices[0]:indices[-1]] if indices.size else audio[0:0] # audio中存放去除头尾空白音频部分，\n",
    "    \n",
    "    slices = []\n",
    "    for i in range(0, audio.shape[0], slice_length):    # 对长语音片段分片，shape数组大小\n",
    "        s = audio[i: i + slice_length]\n",
    "        if s.shape[0] >= min_length:\n",
    "            slices.append(s)            # append在列表尾插入元素，slices中存放分片后的音频信号\n",
    "    \n",
    "    return audio, slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pcm文件转为wav函数:\n",
    "## pcm存储的是int型整数，不含任何采样率相关信息。\n",
    "## wav存储的一般是解码后为[-1, 1]的float数据，文件头有44个字节记录文件的采样率、长度等等信息。\n",
    "def pcm2wav(pcm_path, wav_path, channels=1, bits=16, sample_rate=sr):\n",
    "    data = open(pcm_path, 'rb').read()  # 只读\n",
    "    fw = wave.open(wav_path, 'wb')      # 只写\n",
    "    fw.setnchannels(channels)           # 设置声道数\n",
    "    fw.setsampwidth(bits // 8)          # 设置采样位数\n",
    "    fw.setframerate(sample_rate)        # 设置采样率\n",
    "    fw.writeframes(data)                # 写入数据\n",
    "\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 可视化语音\n",
    "def visualize(index, source='train'):\n",
    "    if source == 'train':\n",
    "        path = train_files[index]\n",
    "    else:\n",
    "        path = vad_files[index]\n",
    "    print(path)\n",
    "\n",
    "    #画出原始信号在时域中具有的形式\n",
    "    audio, slices = load_and_trim(path)\n",
    "    print('Duration: %.2f s' % (audio.shape[0] / sr))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(np.arange(len(audio)), audio)\n",
    "    plt.title('Raw Audio Signal')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Audio Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "    # 获取13维的 MFCC 特征\n",
    "    feature = mfcc(audio, sr, numcep=mfcc_dim)\n",
    "    print('Shape of MFCC:', feature.shape)\n",
    "\n",
    "    # 画出MFCC特征图的频谱图\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized MFCC')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('MFCC Coefficient')\n",
    "    plt.colorbar(im, cax=make_axes_locatable(ax).append_axes('right', size='5%', pad=0.05))\n",
    "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
    "    plt.show()\n",
    "    \n",
    "    # wav_path = 'example.wav'\n",
    "    # pcm2wav(path, wav_path)\n",
    "    \n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(visualize(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 整理数据并查看语音片段的时长分布\n",
    "X_train = []    # 特征的训练集\n",
    "X_vad = []      # 特征的测试集\n",
    "Y_train = []    # 标签的训练集\n",
    "Y_vad = []      # 标签的测试集\n",
    "lengths = []    # 各语音片段长度\n",
    "import time\n",
    "import functools\n",
    "\n",
    "func_mfcc = functools.partial(mfcc, samplerate=sr, numcep=mfcc_dim)\n",
    "\n",
    "localtime = time.localtime()\n",
    "print('%s-%s-%s %0-2s:%0-2s:%0-2s'%(localtime.tm_year, localtime.tm_mon, localtime.tm_mday, \n",
    "                                 localtime.tm_hour, localtime.tm_min, localtime.tm_sec))\n",
    "# 对训练集语音分片后各片段进行MFCC特征提取以及与标签对应\n",
    "for i in tqdm(range(len(train_files))):\n",
    "    path = train_files[i]\n",
    "    audio, slices = load_and_trim(path)\n",
    "    lengths.append(audio.shape[0] / sr)\n",
    "\n",
    "    for s in slices:\n",
    "        X_train.append(func_mfcc(s))\n",
    "        Y_train.append(labels['train'][i])\n",
    "\n",
    "\n",
    "localtime = time.localtime()\n",
    "print('%s-%s-%s %0-2s:%0-2s:%0-2s'%(localtime.tm_year,localtime.tm_mon, localtime.tm_mday, \n",
    "                                 localtime.tm_hour, localtime.tm_min, localtime.tm_sec))\n",
    "\n",
    "# 对测试集语音分片后各片段进行MFCC特征提取以及与标签对应\n",
    "for i in tqdm(range(len(vad_files))):\n",
    "    path = vad_files[i]\n",
    "    audio, slices = load_and_trim(path)\n",
    "    lengths.append(audio.shape[0] / sr)\n",
    "    for s in slices:\n",
    "        X_vad.append(func_mfcc(s))\n",
    "        Y_vad.append(labels['vad'][i])\n",
    "\n",
    "# # 读取保存的已处理过的数据\n",
    "# import pickle as pk\n",
    "# handled_data = {'X_train': X_train, 'Y_train': Y_train, 'X_vad': X_vad, 'Y_vad': Y_vad}\n",
    "# fp = open('saved_data','rb')\n",
    "# handled_data = pk.load(fp)\n",
    "# fp.close()\n",
    "# X_train, Y_train, X_vad, Y_vad, lengths = handled_data['X_train'], handled_data['Y_train'], handled_data['X_vad'], handled_data['Y_vad'], handled_data['lengths']\n",
    "\n",
    "print(len(X_train), len(X_vad))\n",
    "plt.hist(lengths, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存已提取特征的数据到本地\n",
    "\n",
    "import pickle as pk\n",
    "handled_data = {'X_train': X_train, 'Y_train': Y_train, 'X_vad': X_vad, 'Y_vad': Y_vad, 'lengths':lengths}\n",
    "fp = open(r'saved_data', 'wb')\n",
    "pk.dump(handled_data, fp)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对mfcc特征进行归一化\n",
    "samples = np.vstack(X_train)  # vstack()将数据整合到一个数组中\n",
    "mfcc_mean = np.mean(samples, axis=0) # 求均值，axis = 0：压缩行，对各列求均值，返回 1* n 矩阵\n",
    "mfcc_std = np.std(samples, axis=0) # 求标准差\n",
    "\n",
    "X_train = [(x - mfcc_mean) / (mfcc_std + 1e-14) for x in X_train]\n",
    "X_vad = [(x - mfcc_mean) / (mfcc_std + 1e-14) for x in X_vad]\n",
    "\n",
    "maxlen = np.max([x.shape[0] for x in X_train + X_vad])\n",
    "X_train = pad_sequences(X_train, maxlen, 'float32', padding='post', value=0.0)\n",
    "X_vad = pad_sequences(X_vad, maxlen, 'float32', padding='post', value=0.0)\n",
    "print(X_train.shape, X_vad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #对分类标签进行处理\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)  #用0，1，2，3...表示不同类别\n",
    "Y_vad = le.transform(Y_vad)\n",
    "print(le.classes_)\n",
    "\n",
    "class2id = {c: i for i, c in enumerate(le.classes_)}  #标签与数字之间对应关系\n",
    "id2class = {i: c for i, c in enumerate(le.classes_)}\n",
    "\n",
    "num_class = len(le.classes_)\n",
    "Y_train = to_categorical(Y_train, num_class) #将类别向量转化为独热编码的矩阵向量\n",
    "Y_vad = to_categorical(Y_vad, num_class)\n",
    "print(Y_train.shape, Y_vad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义产生批数据的迭代器\n",
    "batch_size = 16\n",
    "\n",
    "def batch_generator(x, y, batch_size=batch_size):\n",
    "    offset = 0\n",
    "    while True:\n",
    "        offset += batch_size\n",
    "\n",
    "        if offset == batch_size or offset >= len(x):\n",
    "            x, y = shuffle(x, y)  #shyffle():对多个等长列表打乱，并且保证一一对应关系不变\n",
    "            offset = batch_size\n",
    "        \n",
    "        X_batch = x[offset - batch_size: offset]\n",
    "        Y_batch = y[offset - batch_size: offset]\n",
    "        yield (X_batch, Y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义模型并训练通过GloblMaxPooling1D对整个序列的输出进行降维，从而变成标准的分类任务\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Multiply, Add, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def conv1d(inputs, filters, kernel_size, dilation_rate):\n",
    "    return Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding='causal', activation=None, dilation_rate=dilation_rate)(inputs)\n",
    "# 批量标准化\n",
    "def batchnorm(inputs):\n",
    "    return BatchNormalization()(inputs)\n",
    "# 激活函数\n",
    "def activation(inputs, activation):\n",
    "    return Activation(activation)(inputs)\n",
    "# 残差块，实现门控激活机制\n",
    "def res_block(inputs, filters, kernel_size, dilation_rate):\n",
    "    hf = activation(batchnorm(conv1d(inputs, filters, kernel_size, dilation_rate)), 'tanh')\n",
    "    hg = activation(batchnorm(conv1d(inputs, filters, kernel_size, dilation_rate)), 'sigmoid')\n",
    "    h0 = Multiply()([hf, hg])\n",
    "\n",
    "    ha = activation(batchnorm(conv1d(h0, filters, 1, 1)), 'tanh')\n",
    "    hs = activation(batchnorm(conv1d(h0, filters, 1, 1)), 'tanh')\n",
    "\n",
    "    return Add()([ha, inputs]), hs #残差层输出和跳连接层输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "num_blocks = 3\n",
    "filters = 128\n",
    "drop_rate = 0.2\n",
    "# model_wt_pth = r'model_wt.index'\n",
    "\n",
    "X = Input(shape=(None, mfcc_dim,), dtype='float32')\n",
    "h0 = activation(batchnorm(conv1d(X, filters, 1, 1)), 'tanh')\n",
    "shortcut = []\n",
    "for i in range(num_blocks):\n",
    "    for r in [1, 2, 4, 8, 16]:\n",
    "        h0, s = res_block(h0, filters, 7, r)\n",
    "        shortcut.append(s)\n",
    "\n",
    "h1 = activation(Add()(shortcut), 'relu')\n",
    "h1 = activation(batchnorm(conv1d(h1, filters, 1, 1)), 'relu') # batch_size, seq_len, filters\n",
    "h1 = batchnorm(conv1d(h1, num_class, 1, 1)) # batch_size, seq_len, num_class\n",
    "h1 = Dropout(drop_rate)(h1)\n",
    "h1 = GlobalMaxPooling1D()(h1) # batch_size, num_class\n",
    "Y = activation(h1, 'softmax')\n",
    "\n",
    "optimizer = Adam(lr=0.01, clipnorm=5) # 原 0.01\n",
    "model = Model(inputs=X, outputs=Y)\n",
    "# 加载权重\n",
    "# if os.path.exists(model_wt_pth):\n",
    "#     print('加载模型权重')\n",
    "#     model.load_weights(model_wt_pth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) # 原loss = 'categorical_crossentropy'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='WaveNet_model.h5', verbose=0)\n",
    "lr_decay = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=1, min_lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    generator=batch_generator(X_train, Y_train),\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=batch_generator(X_vad, Y_vad),\n",
    "    validation_steps=len(X_vad) // batch_size,\n",
    "    callbacks=[checkpointer, lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制损失函数和正确率曲线\n",
    "train_loss = history.history['loss']\n",
    "valid_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(valid_loss, label='valid')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('Loss.jpg')\n",
    "plt.show()\n",
    "\n",
    "train_acc = history.history['acc']\n",
    "valid_acc = history.history['val_acc']\n",
    "plt.plot(train_acc, label='train')\n",
    "plt.plot(valid_acc, label='valid')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('Acc.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存方言与标签之间的映射\n",
    "with open('resources.pkl', 'wb') as fw:\n",
    "    pickle.dump([class2id, id2class, mfcc_mean, mfcc_std], fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "# 绘制正例ROC曲线\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions, pos_label=0)\n",
    "\n",
    "    plt.plot(100 * fp, 100 * tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.xlim([-0.5, 100.5])\n",
    "    plt.ylim([-0.5, 100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(\"./img/multi_roc.png\")\n",
    "\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "pred = model.predict(X_vad)\n",
    "test_predictions = np.argmax(pred, axis=1)\n",
    "true = Y_vad\n",
    "test_scores = 1 - (test_predictions - test_predictions.min()) / (test_predictions.max() - test_predictions.min())\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plot_roc(\"My Model\", X_vad, test_scores, color=colors[0])\n",
    "\n",
    "acc = accuracy_score(true, test_predictions)\n",
    "recall = recall_score(true, test_predictions, average='micro')\n",
    "precision = precision_score(true, test_predictions, average='micro')\n",
    "f1 = f1_score(true, test_predictions, average='micro')\n",
    "print('accuracy: ', acc)\n",
    "print('recall: ', recall)\n",
    "print('precision: ', precision)\n",
    "print('f1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "def get_num_params():\n",
    "    num_params = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        num_params += reduce(mul, [dim.value for dim in shape], 1)\n",
    "    return num_params\n",
    "print(get_num_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d972a04ee18aefd3ce7e0be1db2b657fdd17aab007b8784cfa14884dd53ac62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
